3.Frameing
  - 지도 학습
    -> 해당 모델 학습 'Lable'을 매긴다.
      @Label (라벨 )= 우리가 예측하려는 타겟이 된다.
      @Feature (특성) = 데이터를 표현하는 방식이다.
      @Model = 우리가 예측하려는 모델이다.
        이 모델은 데이터를 통한 학습 과정에서 만들어 볼것이다.

4. Descending into ML
  - 복잡한 학습 방식들이 있고 다양한 모델들이 있다.
    ex) x-y 축의 점이 찍혀있는 상태 (y = Wx + b)
      @W 는 머신러닝에서는 Weight라고 부른다.
      저기서 차원이 늘어날수 있으니, y = W1X1 + b 라고 표현을 하자.

    W는 함수에서 기울기이다. @손실률 이라는 개념이 있다. 평균적으로 그은 직선 모델에서
    기존 분포점과 차이가 손실이다.

  - @회귀 문제에서 손실을 간단히 정의하는 방법이 있다.
    = @L2손실_또는_제곱오차
      = 이 손실은 각 예시별로 정의되는데, 예측값과 실제값의 차를 제곱한 갑시다.
      -> 실제값과 차이가 커질수록 제곱값도 커진다.
  - 모델을 훈련시킬 때는 하나의 예시가 아니라, 전체 데이터 세트에서 손실을 최소화해야한다.

5. Reducing Loss
  <!--  지난 시간에는 손실을 계산하는 법을 알아봤다. -->
  - 그렇다면 손실을 최소화하는 매개변수(w, b값들)은 어떻게 찾아낼까?
  - 매개변수 공간 안에 나아갈 방향이 있으면 좋겠다.
    그래서 초(기)매개 변수 세트를 새로 적용할 때마다 손실이 줄어드는 방향으로.

    -> 이 떄 방향을 잡기 위해서 경사를 계산해 볼 수 있다.
    모델 매개변수를 감안한 손실 함수의 도함수로 가능하다.  

    -> 제곱 손실처럼 간단한 손실 함수의 도함수는 계산하기 쉽다.
    모델 매개변수를 효율적을 업데이트를 할수도 있다. 이 떄 @Iterative_search 를 사용한다.

  @반복적접근 : 입력된 데이터를 기반으로 손실 함수의 경사를 계산한다.
    경사가 음수면 그쪽 방향으로 모델 매개변수를 업데이트 한다.
    그래서 그 방향으로 나아가면서 손실을 줄이려고 한다.
    그런 다음 모델을 다시 업데이트하고 경사를 다시 계산하기를 반복한다.

  - 1차원이라고 가정하면 손실 함수는 2차원함수와 같이 생겼다.
  그러다가 2차원 극소값에 도착하게 되면, 왔던 방향으로 다시 돌아가게 된다.

    -> 그러면 음의 경사 방향으로 한번에 얼만큼 가야하나?
    그것은 학습률에 따라 다르다. 설정할 수 있는 초기 매개변수다.

    *학습률이 아주 작다면, 작은 보폭으로 여러번 이동해서 계산을 여러번 해야 극솟값에 도달할 수 있다.*
    반면에 학습률이 크면, 더 큰 보폭(@step)으로 가고, 극솟값이 더 커지는 방향으로 갈 수가 있고
    손실이 더 커지는 지점에 다 다들수도 있다.

    높은 차원의 모델에서는 발산(손실이 더욱 커지는 경우)이 발생할 수도 있다.
    그렇게 되면, 학습률의 자릿수를 줄여야한다.

  - 지금까지 @경사하강법이라는 알고리즘이었다.
    한 지점에서 출발해 극솟값에 닿길 바라며, 점점 이동해간다. 그러면 이때 출발점이 중요한가?

  - 대부분의 머신 러닝 문제는 위와 같은 볼록(@convex)함수가 아닐 때가 많다. 특히 신경망은 모양이
    볼록하지 않기로 유명하다. 사실 신경망은 그릇모양이라기 보다는 계란판처럼 생겼다.
    여러개의 극솟값이 있고 그 중에 유용한 값도 있고 아닌것도 있고, 그래서 초깃값이 중요하다는 이야기다.

  - 효율성(@efficiency)
    손실 함수에서 경사를 계산할 때, 수학적인 방식을 사용하면 데이터 세트 안에 있는 모든 예시의 경사를
    구해야한다. 그래야만 제대로 된 경사 방향을 따라갈 수 있으니깐.

    ~ 그런데, 몇 십억이 되는 대규모 데이터 세트에서는 이동할 때마다 계산을 아주 많이 해야한다.
      하지만, 전체 데이터 세트가 아니라 하나의 예시에서만 손실의 경사를 계산해도 대체로 괜찮다는 것이
      입증이 되었다.
    - 이 방법을 쓰면 더 큰 보폭으로 이동해야 하지만, 해결책에 도달하는 데 드는 전체 계산량은 훨씬 줄어
      들게 된다. 이러한 방법을 @확률적_경사하강법 (stochastic gradient descent) 이라고 한다.
      실무에서는 이 둘의 중간쯤에 있는 방법을 사용한다. 하나의 예시 또는 전체 데이터 세트가 아닌
      10 ~ 1,000개 정도의 예시가 포함된 배치를 사용하는 것이다. 이를 @미니_배치_경사하강법 이라고한다.

5. First Step With TensorFlow
  - 텐서플로우는 머신러닝 용이다. 그렇지만 그래프 기반의 계산 프레임워크로 무엇이든 인코딩할 수 있다.
  -  고급 API 모델들이 있는데, 그중에서도 @신경망모델 을 쉽게 개발할 수 있게 해주는 *TensorFlow
    Estimators API*를 많이 사용할 것이다.*

6. Generalization
  - 머신러닝을 하려면 *학습 데이터를 모델에 맞추는 것* 만으로는 충분하지 않다.
    머신러닝에서는 @일반화 가 정말 중요하다.

  - 좌표 평면상에 Spam인 것과 Spam이지 않은 것들이 찍혀 있을 때 대략적으로 스팸여부를 구분하는
    선형 모델을 분리 지을 수 있다. 하지만 선형 모델을 어떻게 그어보아도 결국 약간의 오류가 생긴다.
    (약간 삐져나간 것들과 약간씩 넘어온것)

    ~ 그러면 @정확성 100%인 학습 데이터를 얻기 위해서는 조금 더 @복잡한선 을 그으면 된다.
      아메바 처럼.
    ~ *여기서 문제는 무엇인가?*
      => 문제는 새로운 사례가 들어올 경우이다. 저 아메바 안에 새로운 예시가 들어온다면?
        그랬을때 이 모델은 학습 데이터에 @과적합 되어있고, 이전에는 나타나지 않았던 새로운 데이터를
        분류하는 데 적합하지 않게 된다.
  - 모델이 지나치게 복잡한 것, @과적합
    ~ 모델이 과적합이 발생하지 않게 하려면 어떻게 해야하나요?
    (MT) 평균에 벗어나면 잘라내야지.

    => 한 걸음 물어나서 생각해서, *머신러닝이 무엇을 하려고 하는지 생각을 해보자*
    추상적으로 우리에게 데이터를 제공하는 어떤 숨겨진 프로세스가 있다고 볼 수 있다.
    예를 들어, 이 이메일이 스팸인지 아닌지를 사람이를 인식하고 분류하는 과정들이 숨겨진 프로세스이다.
    또는, 강아지 사진이 만들어지고 난 다음 사람들이 이를 귀여움, 귀엽지 않음으로 분류하는 프로세스일
    수도 있다.

  - 데이터가 생성되는 모든 과정들이 이러한 프로세스가 될 수 있다. 우리가 이 프로세스를 완전히 알게
    되지는 않는다. 우리가 알게되는 것은 그러한 프로세스에서 추출한 구체적인 사례들뿐이다.
    이것 우리는 @데이터_샘플 이라고 한다.

  - 데이터 샘플로 모델을 학습시킨 다음 숨겨진 데이터 분포에서 새로운 사례를 가져왔을 때 좋은 결과를
    낼 수 있으면 가장 좋지. 그 결과를 어떻게 신뢰할 수 있을까? 일반화(=단순화시키는 행위)

  - *실제적으로는 경험적인 접근 방식을 사용하기 때문에* 일반화 이론을 사용할 필요가 없다.
  즉, 데이터 분포에서 새로운 사례를 뽑아 모델을 시험해 보는것이다. 이를 @테스트_세트_방법 이라고한다.
  데이터 분포에서 하나의 데이터 사례를 추출하여 모델을 학습시킨다. 이게 @학습세트가 된다.
  그런 다음 사례를 하나 더 추출하여 테스트 세트를 만든다. 학습 세트에서만큼은 테스트 세트에서도
  좋은 예측값을 얻으면 이전에 보지 못 했던 새로운 데이터도 제대로 일반화할 수 있다.

  - 주의할 점.
    1) 데이터 분포에서 사례를 추출할 때는 독립적, 개별적으로 추출해야한다는 점.
      어떤 식으로는 편햔(@bias)되어서는 안 된다.
    2) 이 데이터 분포가 @정상성 을 보이며, 시간이 지나도 변하지 안항야 한다는 점이다.
    3) 항상 동일한 데이터 분포를 사용해야 하며, 갑자기 새로운 분포에서 사례를 추출하면 안 된다.

  - 이러한 가정들이 매우 중요하며, 지도 머신 러닝의 기초가 된다.
    예외가 생길수도 있다. 휴가 시즌과 여름 시즌에 변화를 보이는 사용자 쇼핑 행동과 같은 경우
    @정상성 가정이 지켜지지 않을 수 있다. 사람들이 갑자기 다른 종류의 강아지도 이전에 본 강아지만큼
    귀엽다고 생각하면 동일한 데이터 분포에서 추출한다는 가정도 바뀔 수도 있다. 유행은 변하니깐.

8. Trainning and Test Sets
  - @테스트세트 와 @학습세트 방식에 대하여
  - 만약 사용할 수 있는 데이터 세트가 하나밖에 없다면 어떻게 될까? 숨겨진 분포에서 큰 데이터 세트를
  추출해냈다고 해보자. 이 큰 데이터 세트를 작은 세트 2개로 나눈 다음 각각 학습 및 테스트용으로
  쓸 수 있을 것이다. 데이터 세트를 나누기 전에 한 쪽은 여름, 한쪽은 겨울과 같이 편중되지 않도록
  임의로 데이터를 추출하는 것도 중요하다.

  - 그렇다면 '두 데이터 세트의 크기'를 어떻게 맞춰야 하나? 하나가 커지면 다른 하나가 작아지는데?
    *-> 학습 세트가 커지면 학습 모델이 우수해진다. 테스트 세트가 커지면 평가 측정 항목의 신뢰도가
    높아지고 신뢰 구간의 간격도 좁아진다.*
  - 데이터 세트가 크면 좋다. 표본이 10억 개 정도 있으면 그 중 10~15%만 테스트에 사용해도 학실한
  신뢰 구간을 얻을 수가 있다. 데이터 세트가 아주 작으면 교차 검증처럼 더욱 정교한 방법을 써야 한다.

  - 테스트 데이터를 학습에 사용해서는 안 된다. 왜냐하면 *자신의 모델이 너무 뛰어나다고 믿어버리게 된다.*
  (다 맞아 떨어지게 되므로, 테스트 데이터의 정확성이 100% 나오므로)

9. Validation
  - 강력한 테스트 세트와 학습 세트를 구축한 상태라고 하자.
  실제로 사용한다고 해보자. 우리는 이 분리과정을 몇 차례 반복을 할 것이다.

  학습 데이터로 모델을 학습시키고 테스트 데이터로 테스트한 다음 측정 항목을 관찰할 것이다.

  그러고 나서 @학습률 등의 설정을 약간 조정하고 테스트 세트의 정확성이 개선되었는 지 확인할 것이다.
  몇몇 변수를 추가하거나 빼는 작업을 반복해 테스트 세트 측정 항목을 기반으로 최적 모델을 찾을 것이다.

  -> 여기 어떤 문제가 생길 수 있을까?
    이 테스트 데이터만의 특성에 과적합한 모델이 나올수도 있는데, 그러면 문제가 되겠지만
    이를 피할 방법이 있다. @검증데이터, 라는 3번째 데이트 세트를 만드는 것이다.
    @데이터세트 = 학습 데이터 + 테스트 데이터 + 검증 데이터

  - 학습 데이터로 학습시키고 데이터 평가에는 검증 데이터만 사용하는 약간 개선된 반복 방식을 사용한다.
  즉 테스트 데이터는 사용 안한다. 그리고 검증 데이터에서 좋은 결과가 나올 때까지 반복하고 매개 변수와
  모델을 수정하는 작업을 계속 한다.

  - 그런 다음 최종 테스트 데이터를 대상으로 모델을 테스트한다. 이 때 테스트 데이터에서 얻은 결과가
  검증 데이터 결과와 일치하는 지 확인하게 된다. 일치하지 않는다면 검증 세트에 과적합한 모델일 가능성이
  높다는 뜻이다.

10. Representation
  - *우리가 실제로 작업하게 될 데이터는 형식이 잘 정리된* @특성벡터 *가 아니다.*
    데이터베이스의 기록이나 프로토콜 버퍼와 같은 것이다, 어떤것이든.
    그래서 우리는 이렇게 다양한 성질을 가진 @데이터소스 에서 데이터를 가져와 @특성벡터 를 만들어야한다.

  - 원시 데이터에서 특성을 추출하는 과정을 @특성추출 이라고 한다.
    머신 러닝 개발자는 개발 시간 주우 75% 정도를 특성을 추출하는 데 사용할 정도로 특성은 중요한
    부분이다.

    -> 특성은 어떻게 추출하는가?
    집에 있는 방의 개수와 같이 실제 값으로 표현할 수 있는 기록은 특성 벡터로 사용할 수가 있다.
    아주 자연스럽게도. 하지만, '메인 스트리트'처럼 @문자열이 있다면 어떻게 해야하나?

    문자열 값이 있는 경우, @원-핫 인코딩을 사용해 특성 벡터로 바꾸는 경우가 많다.
    원-핫 인코딩에서는 모든 문자열마다 각각 고유한 계수를 가진다. 예를 들어, 거리 이름에 원-핫
    인코딩을 사용하면 각 거리마다 고유 계수가 생기게 된다.

    '메인 스트리트'와 같으 거리가 있다면, 이 거리 이름에는 1을 다른 모든 항목에는 0을 준다.
    이것을 문자열을 '표현할' 특성 벡터로 사용할 것이다. 원-핫 인코딩은 이렇게 분산되어 있는
    분류 데이터에 적합하다.

  - 좋은 @Feature (특성)
    (1) 우선 특성은 데이터 세트에서 0이 아닌 값으로 최소 몇 차례 이상 나타나야 한다. 어떤 특성이
    0이 아닌 값을 가지지만 매우 드물거나 딱 한 번만 나타난다면 이것은 좋은 특성이 안디ㅏ.
    사전 처리 단계에서는 빼는 게 좋다.

    (2) 특성은 분명하고, 명확한 의미를 가져야 한다.
    그래야 정상 여부 검사 및 디버깅을 통해 특성이 제대로 처리되고 있는 지 확인할 수 있다.
    예를 들어, 주택 수명의 경우 유닉스 에포크 형식을 사용해 초 단위로 표시하는 것보다는 년 단위로
    표시하는 것이 훨씬 쉽다.

    (3) 특성은 특이한 값을 가지면 안 된다.
    어떤 집이 며칠 동안이나 시장에 매물로 나와 있었는지 표시해주는 특성은 좋은 특성이 아니다.
    어떤 집이 매물로 나온 적이 없음을 표시하려면 그냥 (-1)이라는 특수한 값을 써야한다.

    (4) 시간이 지남에 따라 변하면  안된다.
    변화가 일어날 수 있는 업스트림 모델이 그럴 수는 있다. 이 경우 하나의 버전으로 만들어 특별하게
    간주하거나 잘 정의된 상수 의미론이 사용된 모델에서 값을 가져오는 것이 좋다.

    (5) 특성은 이상점 값을 가져서는 안 된다.
    *캘리포니아 주택 데이터에서 총 객실 개수를 총 인구수로 나눠 1인당 방 개수와 같은 합성 특성을
    만들어 볼 때, 그러면 대부분 도시의 한 블록에 1인당 0에서 3,4개 정도의 방이 해당한다는 적당한
    값이 나올 것이다.

    하지만 몇몇 블록의 경우 1인당 50개라는 높은 값이 나왔다. 이건 말이 안 된다. 이 사람들은
    호텔에 몰려 있는 걸까? 이럴 때는 특성을 제한하거나 변환하여 이상점 값을 제거해야 한다.*
    <!--

        가령 인구수와 주택수-아파트 방 수에 대한 정보, 공원의 수를 인구수로 나눠버리면 얼마나
        쾌적한 공간인지 그런 정보들을 뽑아낼 수 있겠지

      -->
    @비닝_Binning 이라는 방법을 생각해 볼 수도 있다.
    캘리포이나 주택 가격에 위도가 미치는 영향을 생각해보자. 위도가 바뀌어도 주택 가격에 직접적인
    영향을 주는 선형 관계는 없다. 하지만 특정 위도 내에서는 강한 상관 관계가 있는 경우가 많다.
    따라서 위도를 몇 개의 작은 빈(@bin)으로 나눌 수 있다. 각 빈은 부울 특성이 되며, 여기에
    원-핫 인코딩을 사용할 수 있다. 그러면 지도상에서 샌디에이고를 나타내는 빈이 1이 될 수 있고
    또는 사우스베이 지역에 있는 빈이 1, 그 외의 모든 지역은 0을 나타낸다.

    이를 통해서 특수한 방법을 사용하지 않아도 간단하게 비선형성을 모델에 매핑할 수 있게 된다.

    @주의 사항
    (1) 데이터의 특성을 아는 것은 중요하다. 머신 러닝을 블랙박스와 같이 취급하는 것은 좋은 습관이 아니다.
    데이터를 제대로 살펴보지도 않고 집어넣은 다음 좋은 결과를 기대할 수는 없다. 데이터에 특이 사항이
    있을 수 있기 때문이다. *그래서 히스토 그램이나 분산형 플롯, 여러 가지 순위 측정 항목 등을 사용하여
    데이터를 시각화해 보자. 다양한 데이터 디버깅을 시도할 수도 있지만 중복값, 누락값, 이상점 등을 찾다가
    많은 시간을 보내게 된다.*

    (2) 시간의 경과에 따라 데이터를 모니터링하는 것이 좋다.
    시간의 경과에 따라 특성의 안전성을 추가로 모니터링하면 시스템을 더욱 견고하게 만들 수 있다.

11. Feature Crosses
  - 선형 문제부터 보자.
    @선형문제 : 여러 입력 변수에 따라 스팸과 스팸이 아닌 항목을 구분하도록 선을 긋는 문제.
    여기에는 x1과 x2라는 2개의 입력 변수가 있다. 그리고 선형 모델인 w1x1, w2x2와 바이어스 항이 있다.
    >> y = sign(b + w1x1 + w2x2)

    여기에서는 선을 긋이 쉽다. 그러나 2차원 좌표 평면에서 1,3 사분면, 2,4분면 이런 경우라면
    단순한 모형으로는 50%이상의 정확성을 얻을 수 없다. 어떻게 해야하나.

    @추가변수 를 정의하는 게 한 가지 방법이다. 이것을 @합성특성 또는 @특성교차_FeatureCross 라고하고
    X3이라고 부르겠다. x3 = x1 * x2 라고 정의를 해보자. x3를 선형 모델에 사용할 수 있을 까
    당연히 할수 있다. x3에 대한 계수 w3을 추가해보자.
    >> y = sign(b + w1x1 + w2x2 + w3x3)

    x1과 x2가 둘가 양수거나 음수면 이 둘의 곱은 항상 양수이다. 따라서 x1과 x2의 곱이 양수면 파란색 점이
    나오게 된다. 이렇게 교차 곱이라는 간단한 @합성특성 을 사용하면 선형 모델 내에서 비선형성을 학습시킬 수
    있다.

  - @특성교차곱 = 다른 변수의 곱으로 합성 특성을 만드는 과정.
  이러한 변수는 A 곱하기 B 라는 형태의 템플릿으로 생각할 수 있고, 조금 더 복합한 형태를 띌 수도 있다.

  - 특성 교차를 유용하게 사용할 수 있는 다른 예
  캘리포니아 집값을 예측할 때 교차 위도나 빈 위도와 함께 침실 수를 감안한다. 이를 통해 같은 침실 3개짜리
  집이라도 샌프란시스코와 새크라멘토에서 집값이 매우 다르다는 점을 알 수 있게 된다.

  -*왜 특성 교차는 필요한 것일까?*
  특성 교차를 사용하면 선형적 학습자에 비선형적 학습을 통합할 수있다는 점이 가장 큰
  장점이다. 선형 학습자는 거대한 데이터 세트에 자연스럽게 적용할 수 있기 때문에
  매우 흥미로운 대상이다. 선형 학습자는 지난 수년간 수십억, 수천억에 도달하는데이터 세트에 적용할 수 있는
  유일한 방법이었다.

  - 하지만 @이제_확장이_쉬운-심층신경망 이 생겼고 이에 관해서는 나중에 살펴볼것이다.

  - 특성 교차 곱 및 심층 신경망을 사용한 선형 학습 효과를 결합하면
  매우 강력한 모델링 도구가 될 수 있다는 흥미로운 연구 결과가 나왔다.


12. Regulation for Simplicity
  - *머신 러닝 효율을 높이기 위한 두번째 요소*
  @학습_손실을_최소화하는_방법 = 사례를 제대로 설정하는 방법
  @정규화가_다음_방법이다.

  - 내가 사용하는 사례를 너무 믿지 않는 것이다. 과적합이라고 부르는 이 곡선에서 볼 수 있듯이
  학습을 매회 반복할 수록 학습 손실이 줄어든다. 손실이 계속 감소하게 된다.

  우리의 목표가 학습 손실을 줄이는 것이니깐. 그러므로 당연히 파란 곡선은 계속 내려가고 결국에는
  아래에서 수렴하게 된다. 반면, 빨간 선은 올라가게 되는데, *이 빨간 선에 주목해야한다.*

  지급은 학습용 사례를 사용해 학습하고 있지만, 새 사례, 즉 시험용 사례를 사용할 떄도 손실이
  적게 발생하길 바라기 때문이다. 빨간 선이 오르지 못하게 하려면 어떻게 해야할까?
  여기서 발생하는 현상은 무엇일까?

  @과적합, 파란선의 사례에서 좋은 값을 보여주고 있기 때문에 여기에서 사용된 사례의 고유한 굴곡
  이나 특성에 @익숙해지기 시작한 것이다. 예를 들어서, 영어를 배우려고 하는데 영어를 가르쳐 줄
  수 있는 사람이 중학생 한 명만 있는 것과 같은 것이다. 처음에는 영어를 배우는 데 많은 도움이
  되고 영어 실력도 많이 늘테지만. 중학생의 언어에 익숙해지고, 일반적인 사람들과 영어로 대화하기
  어려워진다.

  @정규화, 과적합을 피하기 위해서 사용하고 여러가지 전략이 있다.
  그 중하나는 1) @조기중단, 즉 학습 데이터에 수렴하기 전에 학습을 멈추는 것이다. 빨간 곡선의
  진짜 원인을 찾아내려는 방법인데, 실제로 하긴 조금 어렵지만 많이 쓰이는 방법이다.
  2) 모델 복잡성에 @패널티 를 주는 정규화 전략도 있다.

  @패널티-전략
  학습하는 동안 모델 복잡성에 패널티를 준다. 지금까지 학습 사례를 제대로 추출하고 경험적 위험을
  최소화하는 데에만 초점을 맞췄다. 두번째 조항으로 패널티를 줄 것이다. 이를 @구조적-위험-최소화
  라고 한다. 학습 데이터를 제대로 설정함과 동시에 이를 지나치게 신뢰하지 않는다는 두 가지 핵심
  요소 사이에서 균형을 잡아야 한다. 균형 문제로 인해서 모델이 복잡해지기 때문이다.

  @모델-복잡성,을 우리는 어떻게 정의를 할수가 있을까?
  이를 위한 방법으로는 여러가지가 있다. 1) 작은 가중치를 사용하는 방법이 있다. 학습 사례를
  추출하면서 매개 변수를 최대한 작게 만드는 것이다. 이것을 몇 가지 수학적인 방법으로 인코딩
  할 수가 있다. 오늘은 능형 정규화 또는 @L2_정규화 를 살펴보려고 한다.

    @L2정규화, 가중치 제곱값의 합계에 패널티를 준다.베이지안 사전확률에 따라 우리는 학습용
    사례를 보기 전에 이미 우리가 사용하는 가중치가 작고 0을 중심으로 정규분포를 이룰 것으로
    예상하고 있다.

    이것을 사용함에 있어 학습 데이터에 주의를 기울이면서도 필요 이상으로 큰 가중치를 사용하지
    않도록 노력해야 한다. 수학적을 정의를 해보자. 학습을 최적화할 때는 두 개의 항이 있다.

    첫번째는, 1) @학습손실 이다. 사례를 올바르게 추출해야한다. @L항, 즉 손실항은 학습
    데이터에 따라 달라진다. 하지만 이제 모델 @복잡성에서 두 번째 항을 추가했다.
    여기서 위는 제곱된 가중치의 합인 L2 정규화를 사용하게 된다. 두번째 항은 데이터에 의존하지
    않으면서 더 단순한 모델을 만드는 것을 목표로 한다. 이 두항은 람다에 의해 균형을 이루게 된다.

      @람다, 사례를 올바로 설정하기와 모델 단순하게 만들기 사이에서 무엇에 더 비중을 두는지
      보여주는 계수이다. 람다로 무엇을 선택할지는 자신한테 달려있ㄱㅗ 문제에 따라서도 달라진다.

    학습 데이터 양이 많고 학습 데이터와 테스트 데이터가 동일한 경우 통계에서는 @IID 라고 하는데
    이 경우, *많은 정규화가 필요하지 않다.* 1부터 6이면 된다. 혹은 아예 필요없을 수도 있다.

    하지만 학습 데이터 양이 적거나 학습 데이터와 테스트 데이터가 다르면 정규화가 많이 필요하다.
    그리고 교차 검증이나 별도의 테스트 세트를 사용한 조정 과정이 필요할 수도 있다.

13. Logistic Regression
  - 동전을 던졌을 때 앞면이 나올 확률을 예측하는 문제가 있다. *다만, 동전이 약간 구부러져 있다.*
  구부러진 각도, 동저의 질량 등 온갖 변수를 사용할 수 있다. 가장 단순한 모델은 어떤 모델일까?

  아마 이전에 사용한 선형 회귀를 사용할 수도 있겠지. 하지만 동전에 특이한 특성이 있을 수도 있다.
  ~ 아마 지금까지 우리 본적이 없는 무거운 동전이라면? 또는 동전이 상당히 구부러져 있다면?
  0~1사이를 벗어나는 예측이 나올 수도 있다. 벗어난다면 이상한 문제가 생긴거다.

  그렇게되면, 우선 예측치가 이상점 값을 무시하도록 설정할 수 있다. 하지만 현재는 모델에
  편향을 적용한 상태이므로 다시 문제가 생긴다. 그러니 올바른 방법은 지금까지 사용하고 있는
  것과는 다른 손실 함수와 예측 방법을 생각해내는 것이다. 우리가 얻은 값이 자연스럽게 0과
  1 사이의 확률로 해석되고 0과 1 사이의 범위를 절대 초과하지 않는 함수.

  이것을 @로지스틱회귀, 라고 한다. 이것을 사용하면 잘 보정된 확률을 얻을 수 있으며 매우 유용
  하게 활용할 수 있다. 이러하 수치를 실제 확률로 사용하거나 서로 곱해 예측값을 얻을 수도 있다.

  - *어떻게 동작을 하는가?*
  우리가 잘 아는 선형 모델인 @시그모이드 함수에 적용해보겠다. 시그모이드는 0과 1사이의 한정된
  값을 제공한다. 점근선이 있어서, 0에도 1에도 절대 도달하지 않는다. 학습할 때는 다른 손실 함수를
  사용한다. 제곱 손실은 여기에 나타나지 않기 때문에.

  여기에는 @로그손실, 이라는 것을 사용하는데, 정보 이론에서 말하는 새넌의 엔트로피 측정과 매우
  비슷하다는 것을 볼 수 있다. 전부 이해할 필요는 없다.
  "막대 중 하나에 가까월 질수록 손실이 빠르게 증가함"을 알 수 있다.
  이 때 점근선은 다시 등장한다. 학습 측면에서 이러한 점근성은 매우 중요하다.
  이러한 점근성 때문에 정규화를 학습 과정에 확실히 포함시켜야 한다.정규화를 사용하지 않으면
  모델이 주어진 데이터 세트에서 손실을 0에 가깝게 만들기 위해 데이터를 더욱 가깝게 맞출 것이다.
  L2 정규화는 가중치가 균형을 잃지 않게 하는 데 매우 중요하다.

  - 선형 로지스틱 회귀가 좋은 이유는 무엇인가?
  빠르고, 학습 및 예측을 효율적을 활 수 있다. 대량의 데이터에 맞게 확장되는 방법이 필요하거나
  *지연 시간이 짧은 예측 방법이 필요한 경우, 선형 로지스틱 회귀를 사용하는 것이 좋다.
  비선형이 필요한 경우, 특성 교차곱을 추가해주면 된다.*

14. Classification (https://www.youtube.com/watch?v=uKSOY0yOHEQ&list=PLeMn7-yQKbMBDYVoZYsJ4K33Vg9VhxVHj&index=14)
  - 회귀에 관해서 이야기를 했는데, 하지만 머신 러닝 모델을 분류에 사용하고 싶은 경우도 있을 것
    인데. A인지 아닌지, 스팸인지 아닌지.

    이제 확률 결과에 고정 임계값을 적용하여 로지스틱 회귀를 분류 기준을 활용할 수 있다.

15. Regulation for Sparsity

16. Intro to Neutral Nets


17. Training Neutral Nets
18. Multi-Classes Neutral Nets

19. Embeddings
20. Production ML systems
21. Static vs Dynamic Trainning
22. Static vs Dynamic Interface
23. Data Dependencies

24. Fariness
25. Cancer Example
26. Literature Example
27. Real World Guidelines
28. Machine Learning Practicum : Image Classifications
